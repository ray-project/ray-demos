{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Ray\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/ray_project.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray is one of the leading open source ML projects. (date accessed: Nov 2, 2022)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "#### What is Ray?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://www.ray.io/\" target=\"_blank\">Ray</a></strong> is an open-source unified compute framework that makes it easy to scale AI and Python workloads.\n",
    "</div>\n",
    "\n",
    "Ray provides the compute layer to scale applications without becoming a distributed systems expert. These are some key processes that Ray automatically handles:\n",
    "\n",
    "* **Orchestration.** Managing the various components of a distributed system.\n",
    "* **Scheduling.** Coordinating when and where tasks are executed.\n",
    "* **Fault tolerance.** Ensuring tasks complete regardless of inevitable points of failure.\n",
    "* **Auto-scaling.** Adjusting the number of resources allocated to dynamic demand.\n",
    "\n",
    "To lower the effort needed to scale compute intensive workloads, Ray takes a Python-first approach and integrates with many common data science tools. This allows ML practitioners to parallelize Python applications from a laptop to a cluster with minimal code changes.\n",
    "\n",
    "#### Distributed computing: a bit of context and project history\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/ai_compute_annotated.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Amount of compute used in the largest AI training runs far outpaces the processing power of individual CPUs, GPUs, and TPUs. Original diagram from [OpenAI](https://openai.com/blog/ai-and-compute/) with overlaid annotations.|\n",
    "\n",
    "Distributed computing is becoming increasingly relevant for modern machine learning systems. OpenAI's recent paper [AI and Compute](https://openai.com/blog/ai-and-compute/) suggests that the amount of compute needed to train AI models has roughly doubled every 3.5 months since 2012.\n",
    "\n",
    "However, distributed systems are hard to program. Scaling a Python application to a cluster introduces challenges in communication, scheduling, security, failure handling, heterogeneity, transparency, and much more.\n",
    "\n",
    "This context drove the development of Ray: a solution to enable developers to run Python code on clusters without having to think about how to orchestrate and utilize individual machines. The same researchers who created Ray at the University of California Berkeley's [RISELab](https://rise.cs.berkeley.edu/) (the successor to the [AMPLab](https://amplab.cs.berkeley.edu/about/) that created [Apache Spark](https://spark.apache.org/) and [Mesos](https://mesos.apache.org/)) founded [Anyscale](https://www.anyscale.com/)——a managed Ray platform that offers hosted solutions for Ray applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Ray characteristics\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/python_first.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/simple_and_flexible_api.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/scalability.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/heterogeneous_hardware.jpeg\" width=\"70%\" loading=\"lazy\">|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|Python first approach|Simple and flexible API|Scalability|Support for heterogeneous hardware|\n",
    "\n",
    "#### Python first approach\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/python_first.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Ray allows you to flexibly compose distributed applications with easy to use primitives in native Python code. This way, you can scale your existing workloads with minimal code changes. Getting started with Ray Core involves just a few key abstractions:\n",
    "\n",
    "1. [**Tasks**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks). Remote, stateless Python functions\n",
    "1. [**Actors**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). Remote, stateful Python classes\n",
    "1. [**Objects**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects). Tasks and actors create and compute on objects that can be stored and accessed anywhere in the cluster; cached in Ray's distributed [shared-memory](https://en.wikipedia.org/wiki/Shared_memory) object store\n",
    "\n",
    "You will learn more about these abstractions in the [Ray Core tutorials](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core).\n",
    "\n",
    "#### Simple and flexible API\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/simple_and_flexible_api.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "##### Ray Core\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/walkthrough.html\" target=\"_blank\">Ray Core</a></strong> is an open-source, Python, general purpose, distributed computing library that enables ML engineers and Python developers to scale Python applications and accelerate machine learning workloads.\n",
    "</div>\n",
    "\n",
    "Acting as the foundational library for the whole ecosystem, Ray Core provides a minimalist API that enables distributed computing. With just a few methods, you can start building distributed apps:\n",
    "\n",
    "* **`ray.init()`**  \n",
    "Start Ray runtime and connect to the Ray cluster.\n",
    "* **`@ray.remote`**  \n",
    "Decorator that specifies a Python function or class to be executed as a task (remote function) or actor (remote class) in a different process.\n",
    "* **`.remote`**  \n",
    "Postfix to the remote functions and classes; remote operations are *asynchronous*.\n",
    "* **`ray.put()`**  \n",
    "Put an object in the in-memory object store; returns an object reference used to pass the object to any remote function or method call.\n",
    "* **`ray.get()`**  \n",
    "Get a remote object(s) from the object store by specifying the object reference(s).\n",
    "\n",
    "*(In the second part of this notebook you will see illustrative example for some of these methods.)*\n",
    "\n",
    "##### Ray AI Runtime (AIR)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain-specific set of libraries that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Ray AI Runtime (AIR) (sometimes referred to as native libraries and ecosystem of integrations) provides higher level APIs that cater to more domain-specific use cases. Ray AIR enables data scientists and ML engineers to scale individual workloads, end-to-end workflows, and popular ecosystem frameworks, all in Python.\n",
    "\n",
    "#### Scalability\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/scalability.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Ray allows users to utilize large compute clusters in an easy, productive, and resource-efficient way.\n",
    "\n",
    "Fundamentally, Ray treats the entire cluster as a single, unified pool of resources and takes care of optimally mapping compute workloads to the pool. By doing so, Ray largely eliminates non-scalable factors in the system. \n",
    "\n",
    "Some examples of successful user stories include the following:\n",
    "* [Instacart](https://www.youtube.com/watch?v=3t26ucTy0Rs) uses Ray to power their large scale fulfillment ML pipline.\n",
    "* [OpenAI](https://twitter.com/anyscalecompute/status/1562136159135973380) trains their largest models (including ChatGPT).\n",
    "* Companies like [HuggingFace and Cohere](https://www.youtube.com/watch?v=For8yLkZP5w) use Ray Train for scaling model training.\n",
    "\n",
    "A notable strength is Ray's [autoscaler](https://docs.ray.io/en/latest/cluster/key-concepts.html#autoscaling) implements automatic scaling of Ray clusters based on the resource demands of an application. The autoscaler will increase worker nodes when the Ray workload exceeds the cluster's capacity. Whenever worker nodes sit idle, the autoscaler will scale them down.\n",
    "\n",
    "#### Support for heterogeneous hardware\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/heterogeneous_hardware.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Heterogeneous systems present new challenges to distribution because each compute unit has its own programming model. Ray natively supports heterogeneous hardware to achieve load balancing, coherency, and consistency under the hood. All you need to do is specify hardware when initializing a task or actor. For example, a developer can specify in the same application that a one task needs 128 CPUs, another task only requires 0.5 GPUs, and an actor requires 36 CPUs and 8 GPUs.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/heterogeneous_hardware_code.png\" width=\"60%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Easily specify amount of resources needed, by using `num_cpus` and `num_gpus`|\n",
    "\n",
    "An illustrative example is the production [deep learning pipeline at Uber](https://www.anyscale.com/ray-summit-2022/agenda/sessions/215). A heterogeneous setup of 8 GPU nodes and 9 CPU nodes improves the pipeline throughput by 50%, while substantially saving capital cost, compared with the legacy setup of 16 GPU nodes.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/uber.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Deep learning pipeline at Uber using a heterogeneous hardware setup with Ray.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ray libraries\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/map.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stack of Ray libraries - unified toolkit for ML workloads.|\n",
    "\n",
    "There are four layers to Ray's unified compute framework:\n",
    "\n",
    "1. Ray cluster\n",
    "1. Ray Core\n",
    "1. Ray AI Runtime (native libraries)\n",
    "1. Integrations and ecosystem\n",
    "\n",
    "#### Ray cluster\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/cluster/getting-started.html\" target=\"_blank\">Ray cluster</a></strong> is a set of worker nodes connected to a common Ray head node. Ray clusters can be fixed-size, or they can autoscale up and down according to the resources requested by applications running on the cluster.\n",
    "</div>\n",
    "\n",
    "Starting at the bottom with a [cluster](https://docs.ray.io/en/latest/cluster/getting-started.html), Ray sets up and manages clusters of computers so that you can run distributed applications on them. You can deploy a Ray cluster on AWS, GCP or on Kubernetes via the officially supported [KubeRay](https://docs.ray.io/en/latest/cluster/kubernetes/index.html) project. \n",
    "\n",
    "Note: [Anyscale](https://www.anyscale.com/), the company behind Ray, builds enterprise-ready AI compute platform for running and managing Ray applications.\n",
    "\n",
    "#### Ray Core\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/walkthrough.html\" target=\"_blank\">Ray Core</a></strong> is an open-source, Python, general purpose, distributed computing library that enables ML engineers and Python developers to scale Python applications and accelerate machine learning workloads.\n",
    "</div>\n",
    "\n",
    "Ray Core is the foundation that Ray's ML libraries (Ray AIR) and third-party integrations (Ray ecosystem) are built on. This library enables Python developers to easily build scalable, distributed systems that can run on a laptop, cluster, cloud or Kubernetes.\n",
    "\n",
    "Expanding on the key abstractions mentioned before:\n",
    "\n",
    "1. [**Tasks**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks). Remote, stateless Python functions.  \n",
    "Ray tasks are arbitrary Python functions that are executed asychronously on separate Python workers on a Ray cluster nodes. Users can specify their resource requirements in terms of CPUs, GPUs, and custom resources which are used by the cluster scheduler to distribute tasks for parallelized execution.\n",
    "\n",
    "2. [**Actors**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). Remote stateful Python classes.  \n",
    "What tasks are to functions, actors are to classes. An actor is a stateful worker, and the methods of an actor are scheduled on that specific worker and can access and mutate the state of that worker. Like tasks, actors support CPU, GPU, and custom resource requirements.\n",
    "\n",
    "3. [**Objects**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects). In-memory, immutable objects or values that can be accessed anywhere in the computing cluster.  \n",
    "In Ray, tasks and actors create and compute on objects. These remote objects can be stored anywhere in a Ray cluster. Object References are used to refer to them, and they are cached in Ray's distributed shared memory object store.\n",
    "\n",
    "#### Ray AI Runtime\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain-specific set of libraries that equip ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Ray AIR is built on top of Ray Core and focuses on distributed both individual and end-to-end machine learning workflows.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR enables end-to-end ML development and provides multiple options to integrate with other tools and libraries from the MLOps ecosystem.|\n",
    "\n",
    "Each of the five native libraries that Ray AIR wraps distributes a specific ML task:\n",
    "\n",
    "1. [**Ray Data**](https://docs.ray.io/en/latest/data/dataset.html)  \n",
    "Scalable, framework-agnostic data loading and transformation across training, tuning, and prediction.\n",
    "    \n",
    "2. [**Ray Train**](https://docs.ray.io/en/latest/train/train.html)  \n",
    "Distributed multi-node and multi-core model training with fault tolerance that integrates with popular training libraries.\n",
    "\n",
    "3. [**Ray Tune**](https://docs.ray.io/en/latest/tune/index.html)  \n",
    "Scales hyperparameter tuning to optimize model performance.\n",
    "\n",
    "4. [**Ray Serve**](https://docs.ray.io/en/latest/serve/index.html)  \n",
    "Deploys models for online inference, with optional microbatching to improve performance.\n",
    "\n",
    "5. [**Ray RLlib**](https://docs.ray.io/en/latest/rllib/index.html)  \n",
    "Distributed reinforcement learning workloads that integrate with the other Ray AIR libraries.\n",
    "\n",
    "#### Integrations and ecosystem libraries\n",
    "\n",
    "Ray integrates with a [growing ecosystem](https://docs.ray.io/en/latest/ray-overview/ray-libraries.html) of the most popular Python and machine learning libraries and frameworks. Instead of trying to impose a new standards, Ray allows you to scale existing workloads by unifying tools in a common interface. This interface enables you to run ML tasks in a distributed way, a property most of the respective backends don't have, or not to the same extent.\n",
    "\n",
    "Here are a handful of integrations to highlight:\n",
    "\n",
    "* **Ray Datasets**\n",
    "  * [Dask](https://docs.ray.io/en/latest/data/dask-on-ray.html)\n",
    "  * [Pandas/Modin](https://docs.ray.io/en/latest/data/modin/index.html)\n",
    "* **Ray Train and RLlib** \n",
    "  * [Tensorflow](https://docs.ray.io/en/latest/train/api.html#ray.train.tensorflow.TensorflowTrainer)\n",
    "  * [PyTorch](https://docs.ray.io/en/latest/train/api.html#ray.train.torch.TorchTrainer)\n",
    "* **Ray Tune**\n",
    "  * [HyperOpt](https://docs.ray.io/en/latest/tune/examples/hyperopt_example.html)\n",
    "  * [Optuna](https://docs.ray.io/en/latest/tune/examples/optuna_example.html)\n",
    "* **Ray Serve**\n",
    "  * [FastAPI](https://www.anyscale.com/blog/ray-serve-fastapi-the-best-of-both-worlds)\n",
    "  * [gradio](https://docs.ray.io/en/latest/serve/tutorials/gradio-integration.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
